"""
–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è Ralph
"""

import os
import sys
import sqlite3
import json
import hashlib
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import utils
from memory.short_term_memory import ShortTermMemory, semantic_chunking
from memory.reranker import ReRanker
from learning import LearningSystem


class LongTermMemory:
    """
    –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ SQLite.
    –•—Ä–∞–Ω–∏—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–¥–∞—á, –∏—Ç–µ—Ä–∞—Ü–∏–π –∏ —Ä–µ—à–µ–Ω–∏–π –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏.
    """
    
    def __init__(self, db_path: str = None):
        if db_path is None:
            db_path = os.path.join(utils.RALPH_DIR, "memory", "long_term.db")
        
        self.db_path = db_path
        os.makedirs(os.path.dirname(db_path), exist_ok=True)
        
        self._init_database()
    
    def _init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ö–µ–º—É –ë–î"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –¢–∞–±–ª–∏—Ü–∞ –∑–∞–¥–∞—á
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS tasks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_hash TEXT UNIQUE NOT NULL,
                task_text TEXT NOT NULL,
                status TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                completed_at TIMESTAMP,
                total_iterations INTEGER DEFAULT 0
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS iterations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                iteration_num INTEGER NOT NULL,
                thoughts TEXT,
                plan TEXT,
                actions TEXT,
                success BOOLEAN NOT NULL,
                error TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES tasks(id)
            )
        ''')
        
        # –¢–∞–±–ª–∏—Ü–∞ —Ä–µ—à–µ–Ω–∏–π
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS decisions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                task_id INTEGER NOT NULL,
                decision_text TEXT NOT NULL,
                context TEXT,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (task_id) REFERENCES tasks(id)
            )
        ''')
        
        # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_task_hash ON tasks(task_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_task_created ON tasks(created_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_iteration_task ON iterations(task_id)')
        
        conn.commit()
        conn.close()
    
    def store_task(self, task_hash: str, task_text: str, status: str = "pending") -> int:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—É—é –∑–∞–¥–∞—á—É.
        
        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT INTO tasks (task_hash, task_text, status)
                VALUES (?, ?, ?)
            ''', (task_hash, task_text, status))
            
            task_id = cursor.lastrowid
            conn.commit()
            return task_id
        except sqlite3.IntegrityError:
            # –ó–∞–¥–∞—á–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø–æ–ª—É—á–∞–µ–º –µ—ë ID
            cursor.execute('SELECT id FROM tasks WHERE task_hash = ?', (task_hash,))
            task_id = cursor.fetchone()[0]
            return task_id
        finally:
            conn.close()
    
    def update_task_status(self, task_hash: str, status: str, completed_at: datetime = None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        if completed_at:
            cursor.execute('''
                UPDATE tasks 
                SET status = ?, completed_at = ?
                WHERE task_hash = ?
            ''', (status, completed_at, task_hash))
        else:
            cursor.execute('''
                UPDATE tasks 
                SET status = ?
                WHERE task_hash = ?
            ''', (status, task_hash))
        
        conn.commit()
        conn.close()
    
    def get_task_status(self, task_hash: str) -> Optional[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT status FROM tasks WHERE task_hash = ?", (task_hash,))
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else None
    
    def store_iteration(self, task_hash: str, iteration_num: int, 
                       thoughts: str, plan: str, actions: str, 
                       success: bool, error: str = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ç–µ—Ä–∞—Ü–∏—é –∑–∞–¥–∞—á–∏"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º task_id
        cursor.execute('SELECT id FROM tasks WHERE task_hash = ?', (task_hash,))
        result = cursor.fetchone()
        if not result:
            conn.close()
            return
        
        task_id = result[0]
        
        cursor.execute('''
            INSERT INTO iterations 
            (task_id, iteration_num, thoughts, plan, actions, success, error)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (task_id, iteration_num, thoughts, plan, actions, success, error))
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –∏—Ç–µ—Ä–∞—Ü–∏–π
        cursor.execute('''
            UPDATE tasks 
            SET total_iterations = total_iterations + 1
            WHERE id = ?
        ''', (task_id,))
        
        conn.commit()
        conn.close()
    
    def store_decision(self, task_hash: str, decision_text: str, context: Dict = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT id FROM tasks WHERE task_hash = ?', (task_hash,))
        result = cursor.fetchone()
        if not result:
            conn.close()
            return
        
        task_id = result[0]
        context_json = json.dumps(context) if context else None
        
        cursor.execute('''
            INSERT INTO decisions (task_id, decision_text, context)
            VALUES (?, ?, ?)
        ''', (task_id, decision_text, context_json))
        
        conn.commit()
        conn.close()
    
    def get_task_history(self, task_hash: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏—Å—Ç–æ—Ä–∏—é –∑–∞–¥–∞—á–∏"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É
        cursor.execute('SELECT * FROM tasks WHERE task_hash = ?', (task_hash,))
        task_row = cursor.fetchone()
        
        if not task_row:
            conn.close()
            return None
        
        task = dict(task_row)
        task_id = task['id']
        
        # –ü–æ–ª—É—á–∞–µ–º –∏—Ç–µ—Ä–∞—Ü–∏–∏
        cursor.execute('''
            SELECT * FROM iterations 
            WHERE task_id = ? 
            ORDER BY iteration_num
        ''', (task_id,))
        
        iterations = [dict(row) for row in cursor.fetchall()]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ—à–µ–Ω–∏—è
        cursor.execute('''
            SELECT * FROM decisions 
            WHERE task_id = ? 
            ORDER BY timestamp
        ''', (task_id,))
        
        decisions = [dict(row) for row in cursor.fetchall()]
        
        conn.close()
        
        return {
            'task': task,
            'iterations': iterations,
            'decisions': decisions
        }
    
    def search_similar_tasks(self, task_text: str, limit: int = 5) -> List[Dict]:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ (–ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º).
        –î–ª—è –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ RAG –≤ ShortTermMemory.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        keywords = [w.lower() for w in task_text.split() if len(w) > 3]
        
        # –ò—â–µ–º –∑–∞–¥–∞—á–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
        cursor.execute('''
            SELECT * FROM tasks 
            WHERE status = 'done'
            ORDER BY created_at DESC
            LIMIT 50
        ''')
        
        all_tasks = [dict(row) for row in cursor.fetchall()]
        conn.close()
        
        # –ü—Ä–æ—Å—Ç–æ–π scoring –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é —Å–ª–æ–≤
        scored_tasks = []
        for task in all_tasks:
            task_words = set(task['task_text'].lower().split())
            score = sum(1 for kw in keywords if kw in task_words)
            if score > 0:
                scored_tasks.append((score, task))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø
        scored_tasks.sort(reverse=True, key=lambda x: x[0])
        return [task for score, task in scored_tasks[:limit]]


class EntityMemory:
    """
    –ì—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏, —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∏ –∫–ª–∞—Å—Å–∞–º–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ—Å—Ç–æ–π JSON-–≥—Ä–∞—Ñ (–¥–ª—è NetworkX –Ω—É–∂–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏).
    """
    
    def __init__(self, graph_path: str = None):
        if graph_path is None:
            graph_path = os.path.join(utils.RALPH_DIR, "memory", "entity_graph.json")
        
        self.graph_path = graph_path
        os.makedirs(os.path.dirname(graph_path), exist_ok=True)
        
        self.graph = self._load_graph()
    
    def _load_graph(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥—Ä–∞—Ñ –∏–∑ —Ñ–∞–π–ª–∞"""
        if os.path.exists(self.graph_path):
            try:
                with open(self.graph_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        
        return {'nodes': {}, 'edges': []}
    
    def _save_graph(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ –≤ —Ñ–∞–π–ª"""
        with open(self.graph_path, 'w') as f:
            json.dump(self.graph, f, indent=2)
    
    def add_file(self, file_path: str, metadata: Dict = None):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ñ–∞–π–ª –≤ –≥—Ä–∞—Ñ"""
        if file_path not in self.graph['nodes']:
            self.graph['nodes'][file_path] = {
                'type': 'file',
                'metadata': metadata or {},
                'added_at': datetime.now().isoformat()
            }
            self._save_graph()
    
    def add_dependency(self, from_file: str, to_file: str, dep_type: str = "imports"):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏"""
        edge = {
            'from': from_file,
            'to': to_file,
            'type': dep_type
        }
        
        if edge not in self.graph['edges']:
            self.graph['edges'].append(edge)
            self._save_graph()
    
    def get_dependencies(self, file_path: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –æ—Ç –∫–æ—Ç–æ—Ä—ã—Ö –∑–∞–≤–∏—Å–∏—Ç –¥–∞–Ω–Ω—ã–π —Ñ–∞–π–ª"""
        return [
            edge['to'] 
            for edge in self.graph['edges'] 
            if edge['from'] == file_path
        ]
    
    def get_dependents(self, file_path: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å—è—Ç –æ—Ç –¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        return [
            edge['from'] 
            for edge in self.graph['edges'] 
            if edge['to'] == file_path
        ]
    
    def find_affected_files(self, modified_file: str) -> List[str]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä—è–º—ã—Ö –∏ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º—ã—Ö.
        """
        affected = set()
        to_check = [modified_file]
        checked = set()
        
        while to_check:
            current = to_check.pop()
            if current in checked:
                continue
            
            checked.add(current)
            dependents = self.get_dependents(current)
            
            for dep in dependents:
                if dep not in affected:
                    affected.add(dep)
                    to_check.append(dep)
        
        return list(affected)
    
    def update_from_imports(self, file_path: str, imports: List[str]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –≥—Ä–∞—Ñ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤"""
        self.add_file(file_path)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
        self.graph['edges'] = [
            edge for edge in self.graph['edges']
            if edge['from'] != file_path
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
        for imp in imports:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ –∏–º–ø–æ—Ä—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π (—Å–æ–¥–µ—Ä–∂–∏—Ç src/tests)
            if 'src' in imp or 'tests' in imp or imp.startswith('.'):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–º–ø–æ—Ä—Ç –≤ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                imp_path = imp.replace('.', '/') + '.py'
                self.add_dependency(file_path, imp_path, "imports")
        
        self._save_graph()


class MemoryManager:
    """
    –§–∞—Å–∞–¥ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –ø–∞–º—è—Ç—å—é.
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç Long-term Memory –∏ Entity Memory.
    """
    
    def __init__(self):
        self.long_term = LongTermMemory()
        self.entity = EntityMemory()
        self.short_term = ShortTermMemory()
        self.reranker = ReRanker()
        self.learning = LearningSystem()
    
    def _rrf(self, results_list: List[List[Dict]], k: int = 60) -> List[Dict]:
        """
        Reciprocal Rank Fusion –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–∑–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤.
        """
        scores = {}
        content_map = {}
        
        for results in results_list:
            for rank, item in enumerate(results):
                content = item.get('content') or item.get('task_text')
                if not content: continue
                
                if content not in scores:
                    scores[content] = 0
                    content_map[content] = item
                
                scores[content] += 1.0 / (k + rank + 1)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å—á–µ—Ç—á–∏–∫—É
        sorted_content = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [content_map[c] for c in sorted_content]

    def remember(self, task_hash: str, task_text: str, iteration_num: int,
                 thoughts: str, plan: str, actions: str, success: bool, 
                 error: str = None, modified_files: List[str] = None):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤–æ –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –ø–∞–º—è—Ç–∏.
        """
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Long-term Memory
        if iteration_num == 1:
            self.long_term.store_task(task_hash, task_text, status="in_progress")
        
        self.long_term.store_iteration(
            task_hash, iteration_num, thoughts, plan, actions, success, error
        )
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–ø—ã—Ç –≤ —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è
        self.learning.record_outcome(task_text, success, plan, error)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Short-term Memory (RAG)
        context_text = f"Task: {task_text}\nThoughts: {thoughts}\nPlan: {plan}\nActions: {actions}"
        if error:
            context_text += f"\nError: {error}"
            
        chunks = semantic_chunking(context_text)
        ids = [f"{task_hash}_{iteration_num}_{i}" for i in range(len(chunks))]
        metadatas = [{"task_hash": task_hash, "iteration": iteration_num, "success": success} for _ in chunks]
        
        self.short_term.add_documents(documents=chunks, metadatas=metadatas, ids=ids)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Entity Memory –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        if modified_files:
            for file_path in modified_files:
                self.entity.add_file(file_path)
    
    def recall(self, task_text: str, max_items: int = 5) -> str:
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π RAG-–ø–æ–∏—Å–∫ –ø–æ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–¥–∞—á –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        """
        # 1. –ü–æ–∏—Å–∫ –≤ Long-term (SQLite - Keyword-ish)
        lt_results = self.long_term.search_similar_tasks(task_text, limit=10)
        
        # 2. –ü–æ–∏—Å–∫ –≤ Short-term (ChromaDB - Semantic)
        st_search = self.short_term.search(task_text, n_results=10)
        st_results = []
        if st_search and st_search['documents']:
            for i in range(len(st_search['documents'][0])):
                st_results.append({
                    'content': st_search['documents'][0][i],
                    'metadata': st_search['metadatas'][0][i]
                })
        
        # 3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RRF
        combined = self._rrf([lt_results, st_results])
        
        # 4. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ —á–µ—Ä–µ–∑ FlashRank
        if combined:
            rerank_docs = []
            for item in combined:
                content = item.get('content') or item.get('task_text')
                rerank_docs.append({"id": str(hash(content)), "text": content, "metadata": item.get('metadata', {})})
            
            reranked = self.reranker.rerank(task_text, rerank_docs)
            top_results = reranked[:max_items]
        else:
            top_results = []
        
        if not top_results:
            return "No similar tasks found in memory."
        
        context = "## Relevant Context from Memory (Hybrid RAG)\n\n"
        for i, res in enumerate(top_results, 1):
            context += f"### {i}. Source: {res.get('metadata', {}).get('task_hash', 'Unknown')}\n"
            context += f"{res['text']}\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–≤–µ—Ç—ã –æ—Ç —Å–∏—Å—Ç–µ–º—ã –æ–±—É—á–µ–Ω–∏—è
        suggestion = self.learning.suggest_approach(task_text)
        if suggestion:
            context += f"### üí° Suggested Approach from previous successes:\n{suggestion}\n\n"
            
        anti_patterns = self.learning.get_anti_patterns(task_text)
        if anti_patterns:
            context += "### ‚ö†Ô∏è Avoid these patterns (failed before):\n"
            for ap in anti_patterns[:3]:
                context += f"- {ap}\n"
            context += "\n"
            
        return context
    
    def mark_task_complete(self, task_hash: str):
        """–û—Ç–º–µ—á–∞–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—É—é"""
        self.long_term.update_task_status(task_hash, "done", datetime.now())
    
    def mark_task_failed(self, task_hash: str):
        """–û—Ç–º–µ—á–∞–µ—Ç –∑–∞–¥–∞—á—É –∫–∞–∫ –ø—Ä–æ–≤–∞–ª–µ–Ω–Ω—É—é"""
        self.long_term.update_task_status(task_hash, "failed", datetime.now())
    
    def get_affected_files(self, modified_file: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞—Ç—Ä–æ–Ω—É—Ç—ã –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º"""
        return self.entity.find_affected_files(modified_file)
